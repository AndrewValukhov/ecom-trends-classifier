{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "801a1c9e-a8e2-4531-967d-62aaa34d931a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset, DatasetDict, concatenate_datasets\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, TrainingArguments, AutoModelForSequenceClassification, Trainer, \\\n",
    "EarlyStoppingCallback, pipeline, set_seed, PretrainedConfig, EvalPrediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67016988-24f4-4dd9-a392-cac58fe15973",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DatasetDict.load_from_disk('data/train_valid_split_prepared')\n",
    "trend_names = pd.read_csv('data/trend_names.csv')['0'].to_list()\n",
    "id2label = {k:v for k, v in enumerate(trend_names)}\n",
    "label2id = {v:k for k, v in id2label.items()}\n",
    "labels = list(label2id.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0d78ac7-57a9-4920-83d6-ec86bc1b81c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'index', 'assessment', 'tags', 'text', 'Долгая доставка', 'Доставка стала долгой', 'Время доставки не соответствует заявленому', 'Регулярные опоздания', 'Не отследить реальное время доставки', 'Курьер на карте', 'Нет доставки по адресу', 'Не предупреждаем об удалении товара', 'Высокая минимальная сумма заказа', 'Сумма заказа меняется во время набора корзины', 'Минимальная сумма заказа', 'Товары с подходящим сроком годности', 'Высокие цены', 'Не довезли товар', 'Товар испорчен во время доставки', 'Просроченные товары', 'Замечания по работе курьеров', 'Не читаем комментарии', 'Спасибо', 'Нет смысла', 'Всё нормально', 'Всё плохо', 'Скидки для постоянных клиентов', 'Больше акций/скидок', 'Скидка/промокод распространяется не на все товары', 'Непонятно как работает скидка', 'Не сработала скидка/акция/промокод', 'Качество товаров', 'Маленький ассортимент', 'Нет в наличии товара', 'Качество поддержки', 'Замечания по работе сборщика', 'Отменили заказ', 'Знание русского языка', 'Привезли чужой заказ', 'Долго на сборке', 'Сравнивают с конкурентами', 'Скидки за опоздание', 'Курьеры отменяют заказ', 'Не тянет на тенденцию', 'Испорченные товары', 'Не нравится интерфейс приложения', 'Приложение зависает', 'Быстрая доставка', 'Условия работы курьеров', 'СберСпасибо', 'Время работы', 'Неудобный поиск', 'Платежи', 'Возврат денег', 'parsed_tags', 'ru_tags', 'marked_labels_str', 'marked_labels', 'n_labels', 'top_label', 'len_text', 'text_mark', '__index_level_0__'],\n",
       "        num_rows: 4160\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Unnamed: 0', 'index', 'assessment', 'tags', 'text', 'Долгая доставка', 'Доставка стала долгой', 'Время доставки не соответствует заявленому', 'Регулярные опоздания', 'Не отследить реальное время доставки', 'Курьер на карте', 'Нет доставки по адресу', 'Не предупреждаем об удалении товара', 'Высокая минимальная сумма заказа', 'Сумма заказа меняется во время набора корзины', 'Минимальная сумма заказа', 'Товары с подходящим сроком годности', 'Высокие цены', 'Не довезли товар', 'Товар испорчен во время доставки', 'Просроченные товары', 'Замечания по работе курьеров', 'Не читаем комментарии', 'Спасибо', 'Нет смысла', 'Всё нормально', 'Всё плохо', 'Скидки для постоянных клиентов', 'Больше акций/скидок', 'Скидка/промокод распространяется не на все товары', 'Непонятно как работает скидка', 'Не сработала скидка/акция/промокод', 'Качество товаров', 'Маленький ассортимент', 'Нет в наличии товара', 'Качество поддержки', 'Замечания по работе сборщика', 'Отменили заказ', 'Знание русского языка', 'Привезли чужой заказ', 'Долго на сборке', 'Сравнивают с конкурентами', 'Скидки за опоздание', 'Курьеры отменяют заказ', 'Не тянет на тенденцию', 'Испорченные товары', 'Не нравится интерфейс приложения', 'Приложение зависает', 'Быстрая доставка', 'Условия работы курьеров', 'СберСпасибо', 'Время работы', 'Неудобный поиск', 'Платежи', 'Возврат денег', 'parsed_tags', 'ru_tags', 'marked_labels_str', 'marked_labels', 'n_labels', 'top_label', 'len_text', 'text_mark', '__index_level_0__'],\n",
       "        num_rows: 463\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b64a87-5e83-4f69-99c0-7d5b5da9d3c1",
   "metadata": {},
   "source": [
    "**Примечания**:\n",
    "- За кадром остались эксперименты с разбивкой на обучающую/валидационную выборку, но лучшее качество на тесте показало обучение на всем датасете. Просто модель видела больше разнообразных данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5286bf4a-ee8f-472c-8f3d-810d38ce9c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "con_data = concatenate_datasets([data['train'], data['validation']])\n",
    "data['train'] = con_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d5ca219-3e44-4b52-937f-1b051bc88849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Unnamed: 0', 'index', 'assessment', 'tags', 'text', 'Долгая доставка', 'Доставка стала долгой', 'Время доставки не соответствует заявленому', 'Регулярные опоздания', 'Не отследить реальное время доставки', 'Курьер на карте', 'Нет доставки по адресу', 'Не предупреждаем об удалении товара', 'Высокая минимальная сумма заказа', 'Сумма заказа меняется во время набора корзины', 'Минимальная сумма заказа', 'Товары с подходящим сроком годности', 'Высокие цены', 'Не довезли товар', 'Товар испорчен во время доставки', 'Просроченные товары', 'Замечания по работе курьеров', 'Не читаем комментарии', 'Спасибо', 'Нет смысла', 'Всё нормально', 'Всё плохо', 'Скидки для постоянных клиентов', 'Больше акций/скидок', 'Скидка/промокод распространяется не на все товары', 'Непонятно как работает скидка', 'Не сработала скидка/акция/промокод', 'Качество товаров', 'Маленький ассортимент', 'Нет в наличии товара', 'Качество поддержки', 'Замечания по работе сборщика', 'Отменили заказ', 'Знание русского языка', 'Привезли чужой заказ', 'Долго на сборке', 'Сравнивают с конкурентами', 'Скидки за опоздание', 'Курьеры отменяют заказ', 'Не тянет на тенденцию', 'Испорченные товары', 'Не нравится интерфейс приложения', 'Приложение зависает', 'Быстрая доставка', 'Условия работы курьеров', 'СберСпасибо', 'Время работы', 'Неудобный поиск', 'Платежи', 'Возврат денег', 'parsed_tags', 'ru_tags', 'marked_labels_str', 'marked_labels', 'n_labels', 'top_label', 'len_text', 'text_mark', '__index_level_0__'],\n",
       "    num_rows: 4623\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a70d308-db68-45fb-842d-8f36473b9eb0",
   "metadata": {},
   "source": [
    "В качестве базовой модели возьмем модель от ВК - 'deepvk/USER-bge-m3', которая в свою очередь является облегченной и дообученной версией одного из лучших энкодеров - https://huggingface.co/BAAI/bge-m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0109d841-d412-423e-8979-fcfc10484352",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_link = 'deepvk/USER-bge-m3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76ad9730-d7f4-4236-885a-953275a82baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_link)\n",
    "\n",
    "def preprocess_data(examples, text_col='text', max_len=128):\n",
    "  # take a batch of texts\n",
    "  text = examples[text_col]\n",
    "  # encode them\n",
    "  encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=max_len)\n",
    "  # add labels\n",
    "  labels_batch = {k: examples[k] for k in examples.keys() if k in labels}\n",
    "  # create numpy array of shape (batch_size, num_labels)\n",
    "  labels_matrix = np.zeros((len(text), len(labels)))\n",
    "  # fill numpy array\n",
    "  for idx, label in enumerate(labels):\n",
    "    labels_matrix[:, idx] = labels_batch[label]\n",
    "\n",
    "  encoding[\"labels\"] = labels_matrix.tolist()\n",
    "  \n",
    "  return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f67e09c8-7208-46bb-9fd0-32f56560ff40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    # print(predictions.shape)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs > threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    accuracy_0 = accuracy_score(y_true[:, 0], y_pred[:, 0])\n",
    "    accuracy_1 = accuracy_score(y_true[:, 1], y_pred[:, 1])\n",
    "    accuracy_2 = accuracy_score(y_true[:, 2], y_pred[:, 2])\n",
    "    accuracy_3 = accuracy_score(y_true[:, 3], y_pred[:, 3])\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy,\n",
    "              'accuracy_0': accuracy_0,\n",
    "              'accuracy_1': accuracy_1,\n",
    "              'accuracy_2': accuracy_2,\n",
    "              'accuracy_3': accuracy_3}\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b41f616-163b-47ea-a3d6-dcc7a32b9987",
   "metadata": {},
   "source": [
    "**Примечания**:\n",
    "- за кадром остались эксперименты с различным форматом текстовых данных. Имея информацию от орагнизаторов, что assessment - это оценка пользователя, добавим ее к отзыву и будем учиться на этом. Этот сценарий показывал наиболее высокую accuracy на тесте.\n",
    "- поэтому выбираем фичу text_mark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "436f820e-16cd-49f0-b136-7d983789ae90",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_COL = 'text_mark'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "406c884e-69ee-45a7-a767-cf0bb0101e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "499105b4857b45f88727f854211923d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4623 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e5edc6a101c40cb8dade5909442dc47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/463 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_dataset = data.map(preprocess_data, batched=True, remove_columns=data['train'].column_names, fn_kwargs={'text_col': TEXT_COL})\n",
    "encoded_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4fd3f73-4b46-417b-8ad4-4b9727521f26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 4623\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 463\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c815b54c-6e66-40f8-82ca-63c07ed9684c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at deepvk/USER-bge-m3 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_link, \n",
    "                                                           problem_type=\"multi_label_classification\", \n",
    "                                                           num_labels=len(labels),\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id,\n",
    "                                                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2974456a-fb5c-4334-838c-43b6b885023f",
   "metadata": {},
   "source": [
    "**Примечания**:\n",
    "- модель училась на 4-х картах 2080."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3288d2d-4e17-4b8f-89ba-b6632846fdd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "MODEL = 'deep_vk_bge' \n",
    "DATA = 'ecom'\n",
    "EPOCHS = 18 \n",
    "lr = 5e-5 \n",
    "BATCH_SIZE = 8 \n",
    "weight_decay = 1e-6\n",
    "TEST_SIZE = 0.1\n",
    "\n",
    "name_fine_tune = f'{MODEL}_{DATA}_{EPOCHS}_{BATCH_SIZE}_{TEST_SIZE}_{lr}_{weight_decay}_{TEXT_COL}'\n",
    "\n",
    "training_args = TrainingArguments( \n",
    "                                  output_dir=name_fine_tune,\n",
    "                                  num_train_epochs=EPOCHS, \n",
    "                                  weight_decay=weight_decay, \n",
    "                                  per_device_train_batch_size=BATCH_SIZE,\n",
    "                                  learning_rate=lr, \n",
    "                                  logging_steps=100,\n",
    "                                  load_best_model_at_end=True,\n",
    "                                  metric_for_best_model=\"accuracy\",\n",
    "                                  save_strategy='steps',\n",
    "                                  eval_strategy=\"steps\",\n",
    "                                  report_to='none',\n",
    "                                  push_to_hub=False,\n",
    "                                  run_name=name_fine_tune,\n",
    "                                  seed=42,\n",
    "                                  data_seed=42,\n",
    "                                  fp16=False\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56c3fa8c-28d4-4ac5-a281-5a0a7988fcac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer( \n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a96a52c-3894-4a7e-94d0-2c97c72f693e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'deep_vk_bge_ecom_18_8_0.1_5e-05_1e-06_text_mark'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_fine_tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47cdc59-68c1-409a-8f68-31347da32dcb",
   "metadata": {},
   "source": [
    "Качество валидационной выборки здесь просто для информации. За указанное количество эпох модель почти выучивает всю обучающую выборку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73a9587d-9af1-4a15-bae4-d2b88c456c78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/valukhov_ao/projects/jupyter_venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='2610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  15/2610 00:10 < 36:10, 1.20 it/s, Epoch 0.10/18]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/jupyter_venv/lib/python3.8/site-packages/transformers/trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/jupyter_venv/lib/python3.8/site-packages/transformers/trainer.py:2178\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2175\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2177\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 2178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   2179\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2181\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39minclude_num_input_tokens_seen:\n",
      "File \u001b[0;32m~/projects/jupyter_venv/lib/python3.8/site-packages/accelerate/data_loader.py:464\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    463\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m send_to_device(current_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_blocking)\n\u001b[0;32m--> 464\u001b[0m next_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_batches:\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m current_batch\n",
      "File \u001b[0;32m~/projects/jupyter_venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/projects/jupyter_venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/projects/jupyter_venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/jupyter_venv/lib/python3.8/site-packages/transformers/data/data_collator.py:271\u001b[0m, in \u001b[0;36mDataCollatorWithPadding.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m--> 271\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mpad_without_fast_tokenizer_warning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[1;32m    280\u001b[0m         batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/projects/jupyter_venv/lib/python3.8/site-packages/transformers/data/data_collator.py:66\u001b[0m, in \u001b[0;36mpad_without_fast_tokenizer_warning\u001b[0;34m(tokenizer, *pad_args, **pad_kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 66\u001b[0m     padded \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# Restore the state of the warning.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m warning_state\n",
      "File \u001b[0;32m~/projects/jupyter_venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3380\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   3377\u001b[0m             batch_outputs[key] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   3378\u001b[0m         batch_outputs[key]\u001b[38;5;241m.\u001b[39mappend(value)\n\u001b[0;32m-> 3380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatchEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/jupyter_venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:224\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    220\u001b[0m     n_sequences \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_sequences\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_sequences \u001b[38;5;241m=\u001b[39m n_sequences\n\u001b[0;32m--> 224\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/jupyter_venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:759\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    756\u001b[0m     value \u001b[38;5;241m=\u001b[39m [value]\n\u001b[1;32m    758\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor(value):\n\u001b[0;32m--> 759\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    761\u001b[0m     \u001b[38;5;66;03m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;66;03m# # at-least2d\u001b[39;00m\n\u001b[1;32m    763\u001b[0m     \u001b[38;5;66;03m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;66;03m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor[None, :]\u001b[39;00m\n\u001b[1;32m    768\u001b[0m     \u001b[38;5;28mself\u001b[39m[key] \u001b[38;5;241m=\u001b[39m tensor\n",
      "File \u001b[0;32m~/projects/jupyter_venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:721\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors.<locals>.as_tensor\u001b[0;34m(value, dtype)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value[\u001b[38;5;241m0\u001b[39m], np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    720\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(value))\n\u001b[0;32m--> 721\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45407cfb-ddd1-4431-9761-3610dda38a20",
   "metadata": {},
   "source": [
    "Обученную модель отправим на hf, чтобы к ней был доступ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed3c864e-36a0-485b-8439-7a01a9db6fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.push_to_hub('bge-ecom-trends-classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d3c12d-70e0-47fe-b380-9d680293fb49",
   "metadata": {},
   "source": [
    "https://huggingface.co/Maldopast/bge-ecom-trends-classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36082b7-cb8c-4eac-92fd-978a65b92350",
   "metadata": {},
   "source": [
    "**Примечания**:\n",
    "\n",
    "Пробовал, но не дало эффекта\n",
    "\n",
    "- За кадром остались также эксперименты другими архитектурами моделей (XLM-Roberta - лучшая);\n",
    "- Другими лосс-функциями - reduction='sum' в BCEWithLogitsLoss, добавление весов к классам, использование focal loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af6df31-8414-470e-b0aa-9d40cb39ad88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
